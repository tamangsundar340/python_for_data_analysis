{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaeda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Retinopathy_on_Colab.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1_BvUiFZlatdaZusk_MYgBF0AdpxXw8-p\n",
    "\n",
    "This is recent Kaggle Cometition.\n",
    "Link here: https://www.kaggle.com/c/aptos2019-blindness-detection/overview\n",
    "\n",
    "You are provided with a large set of retina images taken using fundus photography under a variety of imaging conditions.\n",
    "\n",
    "A clinician has rated each image for the severity of diabetic retinopathy on a scale of 0 to 4:\n",
    "\n",
    "0 - No DR\n",
    "\n",
    "1 - Mild\n",
    "\n",
    "2 - Moderate\n",
    "\n",
    "3 - Severe\n",
    "\n",
    "4 - Proliferative DR\n",
    "\n",
    "Like any real-world data set, you will encounter noise in both the images and labels. Images may contain artifacts, be out of focus, underexposed, or overexposed. The images were gathered from multiple clinics using a variety of cameras over an extended period of time, which will introduce further variation.\n",
    "\n",
    "For the purpose of enabling running this over Colab, I have resized the images to 300x300 pixels and provided two numpy files containing all the data. One with images and other with their labels. If you want access to full resulution image you may download from the kaggle page. The total download size is alomost 9 GB.  \n",
    "\n",
    "**Assignment 9:**\n",
    "1. Split the data (3662 images) into a training and test set.\n",
    "2. Make a CNN to predict the labels for the retinal images.\n",
    "\"\"\"\n",
    "\n",
    "#Force google colab to switch to high ram mode. Run it only once. \n",
    "a = []\n",
    "while(1):\n",
    "    a.append('1')\n",
    "#Also change runtime type to enable GPU from menu.\n",
    "\n",
    "##This block is only for access of files using google drive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "import cv2;\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np;\n",
    "import pandas as pd\n",
    "from random import shuffle;\n",
    "import cv2;\n",
    "from random import shuffle;\n",
    "from tqdm import tqdm;\n",
    "import tensorflow;\n",
    "from tensorflow.keras import layers;\n",
    "from tensorflow.keras import Model;\n",
    "from tensorflow.keras.optimizers import SGD;\n",
    "from tensorflow.keras.callbacks import TensorBoard;\n",
    "IMAGE_SIZE = 300;\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "train_Data_X_File = drive.CreateFile({'id': '1IJRvy-hV3sgEzjJVMnAtieeaASnjClZ1'});\n",
    "train_Data_Y_File = drive.CreateFile({'id': '1_flYvBAJCp-vP0lYWkAEH3NgkS6Oqa9W'});\n",
    "\n",
    "#This block takes 4-5 minutes to load the training data.\n",
    "train_Data_X_File.GetContentFile('train_Data_X.npy');\n",
    "train_Data_X = np.load('train_Data_X.npy', allow_pickle=True)\n",
    "\n",
    "train_Data_Y_File.GetContentFile('train_Data_Y.npy');\n",
    "train_Data_Y = np.load('train_Data_Y.npy', allow_pickle=True)\n",
    "\n",
    "train_Data_X.shape\n",
    "#No of training images, First columns is of images and second is of labels\n",
    "\n",
    "#Show some training images with labels\n",
    "count = 0;\n",
    "Num_of_Images = 20;\n",
    "plt.figure(figsize=(20,20))\n",
    "label = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR'];\n",
    "for i in np.random.randint(1000, size = Num_of_Images):\n",
    "  count = count+1;\n",
    "  plt.subplot(Num_of_Images/4,4, count);\n",
    "  plt.imshow(np.reshape(train_Data_X[i],(IMAGE_SIZE,IMAGE_SIZE,3)));\n",
    "  plt.title(label[int(train_Data_Y[i])]);\n",
    "\n",
    "#Splitting data into training and testing sets\n",
    "num_of_Rows = train_Data_X.shape[0];\n",
    "num_of_columns = train_Data_X.shape[1];\n",
    "training_X = train_Data_X[:int(np.round(num_of_Rows*0.8))]   #80% data into training\n",
    "testing_X = train_Data_X[int(np.round(num_of_Rows*0.8)):]     #20% data into testing\n",
    "training_Y = train_Data_Y[:int(np.round(num_of_Rows*0.8))]   #80% data into training\n",
    "testing_Y = train_Data_Y[int(np.round(num_of_Rows*0.8)):]     #20% data into testing\n",
    "\n",
    "# Input images are of size 300x300x3 in which image pixels(300x300) and 3 for the three color channels: R, G, and B\n",
    "img_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# First convolution extracts 16 filters that are 3x3 Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Second convolution extracts 32 filters that are 3x3 Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Third convolution extracts 64 filters that are 3x3 Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Second convolution extracts 128 filters that are 3x3 Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(128, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Flatten feature map to a 1-dim tensor so we can add fully connected layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Create output layer with a five nodes and softmax activation\n",
    "output = layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "# Create model:\n",
    "model = Model(img_input, output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Compiling the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=0.00005),metrics=['acc']);\n",
    "\n",
    "# Prepare the data for the training by separating X and Y vectors.\n",
    "X_train = np.array([i[0] for i in tqdm(training_X)]);\n",
    "Y_train = np.array([i[0] for i in training_Y]);\n",
    "\n",
    "X_test = np.array([i[0] for i in tqdm(testing_X)]);\n",
    "Y_test = np.array([i[0] for i in testing_Y]);\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "#Fitting training and testing dataset into model\n",
    "Model_fit = model.fit(X_train,Y_train, batch_size =64, epochs = 10,verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "#Evaluating Test Loss and Test accuracy\n",
    "Marks = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy percentage:', 100*Marks[1],\"%\")\n",
    "print('Test loss perentage:', 100*Marks[0],\"%\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Show some random test images with their predictions as title.\n",
    "count = 0;\n",
    "Num_of_Images = 20;\n",
    "plt.figure(figsize=(20,20))\n",
    "label = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR'];\n",
    "for i in np.random.randint(500, size = Num_of_Images):\n",
    "  count = count+1;\n",
    "  plt.subplot(Num_of_Images/4,4, count);\n",
    "  plt.imshow(np.reshape(X_test[i],(IMAGE_SIZE,IMAGE_SIZE,3)));\n",
    "  P = model.predict(X_test[i].reshape(1,IMAGE_SIZE,IMAGE_SIZE,3)) # Prediction of testing images\n",
    "  P = np.array(P);\n",
    "  plt.title(label[int(Y_test[i])]);\n",
    "\n",
    "#Evaluating accuracy, val_accuracy, loss, val_loss\n",
    "acc = Model_fit.history['acc']\n",
    "val_acc = Model_fit.history['val_acc']\n",
    "loss = Model_fit.history['loss']\n",
    "val_loss = Model_fit.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#Plotting Training and Validation accuracy\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.plot(epochs, acc, 'green', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "plt.legend()\n",
    "\n",
    "#Plotting Training and Validation loss\n",
    "plt.figure()\n",
    "plt.title('Training and validation loss')\n",
    "plt.plot(epochs, loss, 'green', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd3186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
